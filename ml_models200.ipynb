{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from plotingfun import *\n",
    "from helper_functions import *\n",
    "# import optuna\n",
    "# from optuna.integration import TFKerasPruningCallback\n",
    "# from optuna.trial import TrialState\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold,GroupKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.838384</td>\n",
       "      <td>3.687221</td>\n",
       "      <td>213.713734</td>\n",
       "      <td>95.093233</td>\n",
       "      <td>5223.568948</td>\n",
       "      <td>0.085301</td>\n",
       "      <td>0.129810</td>\n",
       "      <td>0.159897</td>\n",
       "      <td>0.133512</td>\n",
       "      <td>0.491479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068037</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>-0.030704</td>\n",
       "      <td>-0.048373</td>\n",
       "      <td>0.025466</td>\n",
       "      <td>-0.025351</td>\n",
       "      <td>-0.056618</td>\n",
       "      <td>-0.001922</td>\n",
       "      <td>-0.000657</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.873360</td>\n",
       "      <td>3.210082</td>\n",
       "      <td>206.750686</td>\n",
       "      <td>93.774949</td>\n",
       "      <td>5057.975587</td>\n",
       "      <td>0.091060</td>\n",
       "      <td>0.132019</td>\n",
       "      <td>0.162783</td>\n",
       "      <td>0.126157</td>\n",
       "      <td>0.487981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031062</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>-0.002707</td>\n",
       "      <td>0.063870</td>\n",
       "      <td>0.020179</td>\n",
       "      <td>0.075819</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>0.086650</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.864793</td>\n",
       "      <td>2.494369</td>\n",
       "      <td>206.170497</td>\n",
       "      <td>93.468677</td>\n",
       "      <td>5341.692649</td>\n",
       "      <td>0.093691</td>\n",
       "      <td>0.136101</td>\n",
       "      <td>0.162830</td>\n",
       "      <td>0.115294</td>\n",
       "      <td>0.492085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064203</td>\n",
       "      <td>0.016455</td>\n",
       "      <td>0.107524</td>\n",
       "      <td>0.025673</td>\n",
       "      <td>-0.023887</td>\n",
       "      <td>-0.035143</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>-0.089909</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.861838</td>\n",
       "      <td>1.154725</td>\n",
       "      <td>213.061185</td>\n",
       "      <td>95.159899</td>\n",
       "      <td>6167.674164</td>\n",
       "      <td>0.093310</td>\n",
       "      <td>0.140412</td>\n",
       "      <td>0.159958</td>\n",
       "      <td>0.106273</td>\n",
       "      <td>0.500047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023822</td>\n",
       "      <td>0.043558</td>\n",
       "      <td>0.031559</td>\n",
       "      <td>0.066844</td>\n",
       "      <td>-0.003058</td>\n",
       "      <td>0.037149</td>\n",
       "      <td>-0.037936</td>\n",
       "      <td>0.093463</td>\n",
       "      <td>-0.037793</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.811886</td>\n",
       "      <td>0.548218</td>\n",
       "      <td>220.890395</td>\n",
       "      <td>97.769892</td>\n",
       "      <td>7153.931355</td>\n",
       "      <td>0.095280</td>\n",
       "      <td>0.141574</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.103782</td>\n",
       "      <td>0.502221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017267</td>\n",
       "      <td>-0.033110</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>-0.065029</td>\n",
       "      <td>-0.045097</td>\n",
       "      <td>-0.034140</td>\n",
       "      <td>-0.021747</td>\n",
       "      <td>0.035709</td>\n",
       "      <td>0.014168</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919241</th>\n",
       "      <td>9.126265</td>\n",
       "      <td>-6.747966</td>\n",
       "      <td>112.640620</td>\n",
       "      <td>88.297985</td>\n",
       "      <td>132.365813</td>\n",
       "      <td>0.122106</td>\n",
       "      <td>0.131850</td>\n",
       "      <td>0.136733</td>\n",
       "      <td>0.133890</td>\n",
       "      <td>0.475422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068897</td>\n",
       "      <td>-0.016020</td>\n",
       "      <td>0.017231</td>\n",
       "      <td>-0.048754</td>\n",
       "      <td>0.009488</td>\n",
       "      <td>-0.007705</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>-0.050827</td>\n",
       "      <td>0.036063</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919242</th>\n",
       "      <td>9.202971</td>\n",
       "      <td>-6.231649</td>\n",
       "      <td>98.297751</td>\n",
       "      <td>88.621311</td>\n",
       "      <td>111.474033</td>\n",
       "      <td>0.123212</td>\n",
       "      <td>0.129555</td>\n",
       "      <td>0.137837</td>\n",
       "      <td>0.135352</td>\n",
       "      <td>0.474044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032180</td>\n",
       "      <td>-0.011895</td>\n",
       "      <td>-0.020704</td>\n",
       "      <td>-0.045991</td>\n",
       "      <td>-0.090781</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>-0.069776</td>\n",
       "      <td>-0.049738</td>\n",
       "      <td>0.034317</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919243</th>\n",
       "      <td>9.127710</td>\n",
       "      <td>-6.152886</td>\n",
       "      <td>99.144110</td>\n",
       "      <td>88.588029</td>\n",
       "      <td>91.065046</td>\n",
       "      <td>0.123646</td>\n",
       "      <td>0.127746</td>\n",
       "      <td>0.139256</td>\n",
       "      <td>0.135016</td>\n",
       "      <td>0.474336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166728</td>\n",
       "      <td>-0.041358</td>\n",
       "      <td>0.186003</td>\n",
       "      <td>-0.072199</td>\n",
       "      <td>0.025414</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>-0.001281</td>\n",
       "      <td>0.089995</td>\n",
       "      <td>-0.021268</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919244</th>\n",
       "      <td>9.036572</td>\n",
       "      <td>-6.139675</td>\n",
       "      <td>100.274104</td>\n",
       "      <td>88.686415</td>\n",
       "      <td>73.953992</td>\n",
       "      <td>0.123435</td>\n",
       "      <td>0.126995</td>\n",
       "      <td>0.140578</td>\n",
       "      <td>0.131824</td>\n",
       "      <td>0.477168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164801</td>\n",
       "      <td>-0.075886</td>\n",
       "      <td>0.033956</td>\n",
       "      <td>-0.008692</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>0.053695</td>\n",
       "      <td>-0.048150</td>\n",
       "      <td>0.130825</td>\n",
       "      <td>0.072898</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919245</th>\n",
       "      <td>9.102641</td>\n",
       "      <td>-6.502021</td>\n",
       "      <td>105.759641</td>\n",
       "      <td>89.119262</td>\n",
       "      <td>60.809506</td>\n",
       "      <td>0.121372</td>\n",
       "      <td>0.126958</td>\n",
       "      <td>0.141045</td>\n",
       "      <td>0.125710</td>\n",
       "      <td>0.484915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048732</td>\n",
       "      <td>-0.052517</td>\n",
       "      <td>-0.036422</td>\n",
       "      <td>-0.001697</td>\n",
       "      <td>-0.019609</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>-0.057776</td>\n",
       "      <td>0.041064</td>\n",
       "      <td>0.026267</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>919246 rows × 273 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1           2          3            4         5    \\\n",
       "0       11.838384  3.687221  213.713734  95.093233  5223.568948  0.085301   \n",
       "1       11.873360  3.210082  206.750686  93.774949  5057.975587  0.091060   \n",
       "2       11.864793  2.494369  206.170497  93.468677  5341.692649  0.093691   \n",
       "3       11.861838  1.154725  213.061185  95.159899  6167.674164  0.093310   \n",
       "4       11.811886  0.548218  220.890395  97.769892  7153.931355  0.095280   \n",
       "...           ...       ...         ...        ...          ...       ...   \n",
       "919241   9.126265 -6.747966  112.640620  88.297985   132.365813  0.122106   \n",
       "919242   9.202971 -6.231649   98.297751  88.621311   111.474033  0.123212   \n",
       "919243   9.127710 -6.152886   99.144110  88.588029    91.065046  0.123646   \n",
       "919244   9.036572 -6.139675  100.274104  88.686415    73.953992  0.123435   \n",
       "919245   9.102641 -6.502021  105.759641  89.119262    60.809506  0.121372   \n",
       "\n",
       "             6         7         8         9    ...       263       264  \\\n",
       "0       0.129810  0.159897  0.133512  0.491479  ... -0.068037  0.022072   \n",
       "1       0.132019  0.162783  0.126157  0.487981  ... -0.031062  0.002729   \n",
       "2       0.136101  0.162830  0.115294  0.492085  ...  0.064203  0.016455   \n",
       "3       0.140412  0.159958  0.106273  0.500047  ... -0.023822  0.043558   \n",
       "4       0.141574  0.157143  0.103782  0.502221  ... -0.017267 -0.033110   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "919241  0.131850  0.136733  0.133890  0.475422  ...  0.068897 -0.016020   \n",
       "919242  0.129555  0.137837  0.135352  0.474044  ...  0.032180 -0.011895   \n",
       "919243  0.127746  0.139256  0.135016  0.474336  ...  0.166728 -0.041358   \n",
       "919244  0.126995  0.140578  0.131824  0.477168  ... -0.164801 -0.075886   \n",
       "919245  0.126958  0.141045  0.125710  0.484915  ...  0.048732 -0.052517   \n",
       "\n",
       "             265       266       267       268       269       270       271  \\\n",
       "0      -0.030704 -0.048373  0.025466 -0.025351 -0.056618 -0.001922 -0.000657   \n",
       "1      -0.002707  0.063870  0.020179  0.075819 -0.002547  0.086650 -0.009901   \n",
       "2       0.107524  0.025673 -0.023887 -0.035143  0.028481  0.014399 -0.089909   \n",
       "3       0.031559  0.066844 -0.003058  0.037149 -0.037936  0.093463 -0.037793   \n",
       "4       0.004993 -0.065029 -0.045097 -0.034140 -0.021747  0.035709  0.014168   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "919241  0.017231 -0.048754  0.009488 -0.007705  0.000256 -0.050827  0.036063   \n",
       "919242 -0.020704 -0.045991 -0.090781  0.001906 -0.069776 -0.049738  0.034317   \n",
       "919243  0.186003 -0.072199  0.025414  0.005476 -0.001281  0.089995 -0.021268   \n",
       "919244  0.033956 -0.008692  0.011696  0.053695 -0.048150  0.130825  0.072898   \n",
       "919245 -0.036422 -0.001697 -0.019609  0.007095 -0.057776  0.041064  0.026267   \n",
       "\n",
       "        272  \n",
       "0         6  \n",
       "1         6  \n",
       "2         6  \n",
       "3         6  \n",
       "4         6  \n",
       "...     ...  \n",
       "919241    2  \n",
       "919242    2  \n",
       "919243    2  \n",
       "919244    2  \n",
       "919245    2  \n",
       "\n",
       "[919246 rows x 273 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_csv(\"F:\\\\Group 19\\\\newApproachMATLAB\\\\features\\\\four_channels\\\\features_freq.csv\", header=None)\n",
    "features\n",
    "\n",
    "# label = pd.read_csv(\"F:\\\\Group 19\\\\newApproachMATLAB\\\\features\\\\four_channels\\\\labels.csv\", header=None)\n",
    "# label\n",
    "\n",
    "# names = pd.read_csv(\"F:\\\\Group 19\\\\newApproachMATLAB\\\\features\\\\four_channels\\\\groups.csv\", header=None)\n",
    "# names\n",
    "\n",
    "# data = pd.concat([names,features,label], ignore_index=False,axis=1)\n",
    "# data\n",
    "\n",
    "# data.to_csv('final_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr03-0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tr03-0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr03-0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tr03-0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tr03-0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919241</th>\n",
       "      <td>tr14-0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919242</th>\n",
       "      <td>tr14-0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919243</th>\n",
       "      <td>tr14-0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919244</th>\n",
       "      <td>tr14-0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919245</th>\n",
       "      <td>tr14-0291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>919246 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0       tr03-0005\n",
       "1       tr03-0005\n",
       "2       tr03-0005\n",
       "3       tr03-0005\n",
       "4       tr03-0005\n",
       "...           ...\n",
       "919241  tr14-0291\n",
       "919242  tr14-0291\n",
       "919243  tr14-0291\n",
       "919244  tr14-0291\n",
       "919245  tr14-0291\n",
       "\n",
       "[919246 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = pd.read_csv(\"F:\\\\Group 19\\\\newApproachMATLAB\\\\features\\\\four_channels\\\\groups.csv\", header=None)\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([names,features], ignore_index=True,axis=1)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr03-0005</td>\n",
       "      <td>11.838384</td>\n",
       "      <td>3.687221</td>\n",
       "      <td>213.713734</td>\n",
       "      <td>95.093233</td>\n",
       "      <td>5223.568948</td>\n",
       "      <td>0.085301</td>\n",
       "      <td>0.129810</td>\n",
       "      <td>0.159897</td>\n",
       "      <td>0.133512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068037</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>-0.030704</td>\n",
       "      <td>-0.048373</td>\n",
       "      <td>0.025466</td>\n",
       "      <td>-0.025351</td>\n",
       "      <td>-0.056618</td>\n",
       "      <td>-0.001922</td>\n",
       "      <td>-0.000657</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tr03-0005</td>\n",
       "      <td>11.873360</td>\n",
       "      <td>3.210082</td>\n",
       "      <td>206.750686</td>\n",
       "      <td>93.774949</td>\n",
       "      <td>5057.975587</td>\n",
       "      <td>0.091060</td>\n",
       "      <td>0.132019</td>\n",
       "      <td>0.162783</td>\n",
       "      <td>0.126157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031062</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>-0.002707</td>\n",
       "      <td>0.063870</td>\n",
       "      <td>0.020179</td>\n",
       "      <td>0.075819</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>0.086650</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr03-0005</td>\n",
       "      <td>11.864793</td>\n",
       "      <td>2.494369</td>\n",
       "      <td>206.170497</td>\n",
       "      <td>93.468677</td>\n",
       "      <td>5341.692649</td>\n",
       "      <td>0.093691</td>\n",
       "      <td>0.136101</td>\n",
       "      <td>0.162830</td>\n",
       "      <td>0.115294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064203</td>\n",
       "      <td>0.016455</td>\n",
       "      <td>0.107524</td>\n",
       "      <td>0.025673</td>\n",
       "      <td>-0.023887</td>\n",
       "      <td>-0.035143</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>-0.089909</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tr03-0005</td>\n",
       "      <td>11.861838</td>\n",
       "      <td>1.154725</td>\n",
       "      <td>213.061185</td>\n",
       "      <td>95.159899</td>\n",
       "      <td>6167.674164</td>\n",
       "      <td>0.093310</td>\n",
       "      <td>0.140412</td>\n",
       "      <td>0.159958</td>\n",
       "      <td>0.106273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023822</td>\n",
       "      <td>0.043558</td>\n",
       "      <td>0.031559</td>\n",
       "      <td>0.066844</td>\n",
       "      <td>-0.003058</td>\n",
       "      <td>0.037149</td>\n",
       "      <td>-0.037936</td>\n",
       "      <td>0.093463</td>\n",
       "      <td>-0.037793</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tr03-0005</td>\n",
       "      <td>11.811886</td>\n",
       "      <td>0.548218</td>\n",
       "      <td>220.890395</td>\n",
       "      <td>97.769892</td>\n",
       "      <td>7153.931355</td>\n",
       "      <td>0.095280</td>\n",
       "      <td>0.141574</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.103782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017267</td>\n",
       "      <td>-0.033110</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>-0.065029</td>\n",
       "      <td>-0.045097</td>\n",
       "      <td>-0.034140</td>\n",
       "      <td>-0.021747</td>\n",
       "      <td>0.035709</td>\n",
       "      <td>0.014168</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919241</th>\n",
       "      <td>tr14-0291</td>\n",
       "      <td>9.126265</td>\n",
       "      <td>-6.747966</td>\n",
       "      <td>112.640620</td>\n",
       "      <td>88.297985</td>\n",
       "      <td>132.365813</td>\n",
       "      <td>0.122106</td>\n",
       "      <td>0.131850</td>\n",
       "      <td>0.136733</td>\n",
       "      <td>0.133890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068897</td>\n",
       "      <td>-0.016020</td>\n",
       "      <td>0.017231</td>\n",
       "      <td>-0.048754</td>\n",
       "      <td>0.009488</td>\n",
       "      <td>-0.007705</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>-0.050827</td>\n",
       "      <td>0.036063</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919242</th>\n",
       "      <td>tr14-0291</td>\n",
       "      <td>9.202971</td>\n",
       "      <td>-6.231649</td>\n",
       "      <td>98.297751</td>\n",
       "      <td>88.621311</td>\n",
       "      <td>111.474033</td>\n",
       "      <td>0.123212</td>\n",
       "      <td>0.129555</td>\n",
       "      <td>0.137837</td>\n",
       "      <td>0.135352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032180</td>\n",
       "      <td>-0.011895</td>\n",
       "      <td>-0.020704</td>\n",
       "      <td>-0.045991</td>\n",
       "      <td>-0.090781</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>-0.069776</td>\n",
       "      <td>-0.049738</td>\n",
       "      <td>0.034317</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919243</th>\n",
       "      <td>tr14-0291</td>\n",
       "      <td>9.127710</td>\n",
       "      <td>-6.152886</td>\n",
       "      <td>99.144110</td>\n",
       "      <td>88.588029</td>\n",
       "      <td>91.065046</td>\n",
       "      <td>0.123646</td>\n",
       "      <td>0.127746</td>\n",
       "      <td>0.139256</td>\n",
       "      <td>0.135016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166728</td>\n",
       "      <td>-0.041358</td>\n",
       "      <td>0.186003</td>\n",
       "      <td>-0.072199</td>\n",
       "      <td>0.025414</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>-0.001281</td>\n",
       "      <td>0.089995</td>\n",
       "      <td>-0.021268</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919244</th>\n",
       "      <td>tr14-0291</td>\n",
       "      <td>9.036572</td>\n",
       "      <td>-6.139675</td>\n",
       "      <td>100.274104</td>\n",
       "      <td>88.686415</td>\n",
       "      <td>73.953992</td>\n",
       "      <td>0.123435</td>\n",
       "      <td>0.126995</td>\n",
       "      <td>0.140578</td>\n",
       "      <td>0.131824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164801</td>\n",
       "      <td>-0.075886</td>\n",
       "      <td>0.033956</td>\n",
       "      <td>-0.008692</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>0.053695</td>\n",
       "      <td>-0.048150</td>\n",
       "      <td>0.130825</td>\n",
       "      <td>0.072898</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919245</th>\n",
       "      <td>tr14-0291</td>\n",
       "      <td>9.102641</td>\n",
       "      <td>-6.502021</td>\n",
       "      <td>105.759641</td>\n",
       "      <td>89.119262</td>\n",
       "      <td>60.809506</td>\n",
       "      <td>0.121372</td>\n",
       "      <td>0.126958</td>\n",
       "      <td>0.141045</td>\n",
       "      <td>0.125710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048732</td>\n",
       "      <td>-0.052517</td>\n",
       "      <td>-0.036422</td>\n",
       "      <td>-0.001697</td>\n",
       "      <td>-0.019609</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>-0.057776</td>\n",
       "      <td>0.041064</td>\n",
       "      <td>0.026267</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>919246 rows × 274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1         2           3          4            5    \\\n",
       "0       tr03-0005  11.838384  3.687221  213.713734  95.093233  5223.568948   \n",
       "1       tr03-0005  11.873360  3.210082  206.750686  93.774949  5057.975587   \n",
       "2       tr03-0005  11.864793  2.494369  206.170497  93.468677  5341.692649   \n",
       "3       tr03-0005  11.861838  1.154725  213.061185  95.159899  6167.674164   \n",
       "4       tr03-0005  11.811886  0.548218  220.890395  97.769892  7153.931355   \n",
       "...           ...        ...       ...         ...        ...          ...   \n",
       "919241  tr14-0291   9.126265 -6.747966  112.640620  88.297985   132.365813   \n",
       "919242  tr14-0291   9.202971 -6.231649   98.297751  88.621311   111.474033   \n",
       "919243  tr14-0291   9.127710 -6.152886   99.144110  88.588029    91.065046   \n",
       "919244  tr14-0291   9.036572 -6.139675  100.274104  88.686415    73.953992   \n",
       "919245  tr14-0291   9.102641 -6.502021  105.759641  89.119262    60.809506   \n",
       "\n",
       "             6         7         8         9    ...       264       265  \\\n",
       "0       0.085301  0.129810  0.159897  0.133512  ... -0.068037  0.022072   \n",
       "1       0.091060  0.132019  0.162783  0.126157  ... -0.031062  0.002729   \n",
       "2       0.093691  0.136101  0.162830  0.115294  ...  0.064203  0.016455   \n",
       "3       0.093310  0.140412  0.159958  0.106273  ... -0.023822  0.043558   \n",
       "4       0.095280  0.141574  0.157143  0.103782  ... -0.017267 -0.033110   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "919241  0.122106  0.131850  0.136733  0.133890  ...  0.068897 -0.016020   \n",
       "919242  0.123212  0.129555  0.137837  0.135352  ...  0.032180 -0.011895   \n",
       "919243  0.123646  0.127746  0.139256  0.135016  ...  0.166728 -0.041358   \n",
       "919244  0.123435  0.126995  0.140578  0.131824  ... -0.164801 -0.075886   \n",
       "919245  0.121372  0.126958  0.141045  0.125710  ...  0.048732 -0.052517   \n",
       "\n",
       "             266       267       268       269       270       271       272  \\\n",
       "0      -0.030704 -0.048373  0.025466 -0.025351 -0.056618 -0.001922 -0.000657   \n",
       "1      -0.002707  0.063870  0.020179  0.075819 -0.002547  0.086650 -0.009901   \n",
       "2       0.107524  0.025673 -0.023887 -0.035143  0.028481  0.014399 -0.089909   \n",
       "3       0.031559  0.066844 -0.003058  0.037149 -0.037936  0.093463 -0.037793   \n",
       "4       0.004993 -0.065029 -0.045097 -0.034140 -0.021747  0.035709  0.014168   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "919241  0.017231 -0.048754  0.009488 -0.007705  0.000256 -0.050827  0.036063   \n",
       "919242 -0.020704 -0.045991 -0.090781  0.001906 -0.069776 -0.049738  0.034317   \n",
       "919243  0.186003 -0.072199  0.025414  0.005476 -0.001281  0.089995 -0.021268   \n",
       "919244  0.033956 -0.008692  0.011696  0.053695 -0.048150  0.130825  0.072898   \n",
       "919245 -0.036422 -0.001697 -0.019609  0.007095 -0.057776  0.041064  0.026267   \n",
       "\n",
       "        273  \n",
       "0         6  \n",
       "1         6  \n",
       "2         6  \n",
       "3         6  \n",
       "4         6  \n",
       "...     ...  \n",
       "919241    2  \n",
       "919242    2  \n",
       "919243    2  \n",
       "919244    2  \n",
       "919245    2  \n",
       "\n",
       "[919246 rows x 274 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:51<00:00, 171.96s/it]\n"
     ]
    }
   ],
   "source": [
    "accuracy_scores = []\n",
    "f1_scores_list = []\n",
    "recall_scores = []\n",
    "precision_scores_list = []\n",
    "for i in tqdm(range(1)):\n",
    "    # Generate a list of unique IDs from your features\n",
    "    unique_ids = data[0].unique()\n",
    "\n",
    "    # Randomly select 10 unique IDs\n",
    "    random_ids = pd.Series(unique_ids).sample(90).tolist()\n",
    "\n",
    "    # Filter the features\n",
    "    random_10 = data[data[0].isin(random_ids)]\n",
    "\n",
    "\n",
    "    # Extract the features and target variable from the sample\n",
    "    X = random_10.drop([0,273], axis=1)\n",
    "    y = random_10[273]\n",
    "\n",
    "    # Change labels from 1-index based to 0-index based (reason: features imported from matlab)\n",
    "    y=y-1\n",
    "\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X.fillna(0,inplace=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42,shuffle=False)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    modelxgboost = xgb.XGBClassifier()\n",
    "    modelxgboost.fit(X_train_scaled, y_train)\n",
    "    y_pred = modelxgboost.predict(X_test_scaled)\n",
    "        \n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "        \n",
    "    f1 = f1_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    f1_scores_list.append(f1)\n",
    "        \n",
    "    recall = recall_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    recall_scores.append(recall)\n",
    "        \n",
    "    precision = precision_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    precision_scores_list.append(precision)\n",
    "\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "mean_f1 = np.mean(f1_scores_list)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_precision = np.mean(precision_scores_list)\n",
    "\n",
    "xgboost_score = {\"Accuracy\": mean_accuracy, \"F1-Score\": mean_f1, \"Recall\": mean_recall, \"Precision\": mean_precision}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6243538886885442"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.6243538886885442,\n",
       " 'F1-Score': 0.5193189760340328,\n",
       " 'Recall': 0.5218086475731928,\n",
       " 'Precision': 0.559651149403252}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.09026\n",
      "0:\tlearn: 1.6574329\ttotal: 348ms\tremaining: 5m 47s\n",
      "1:\tlearn: 1.5567122\ttotal: 528ms\tremaining: 4m 23s\n",
      "2:\tlearn: 1.4795557\ttotal: 699ms\tremaining: 3m 52s\n",
      "3:\tlearn: 1.4142991\ttotal: 871ms\tremaining: 3m 36s\n",
      "4:\tlearn: 1.3624878\ttotal: 1.04s\tremaining: 3m 26s\n",
      "5:\tlearn: 1.3094955\ttotal: 1.2s\tremaining: 3m 19s\n",
      "6:\tlearn: 1.2693736\ttotal: 1.37s\tremaining: 3m 14s\n",
      "7:\tlearn: 1.2300371\ttotal: 1.55s\tremaining: 3m 12s\n",
      "8:\tlearn: 1.1948540\ttotal: 1.72s\tremaining: 3m 9s\n",
      "9:\tlearn: 1.1611500\ttotal: 1.89s\tremaining: 3m 7s\n",
      "10:\tlearn: 1.1355491\ttotal: 2.06s\tremaining: 3m 5s\n",
      "11:\tlearn: 1.1061633\ttotal: 2.23s\tremaining: 3m 3s\n",
      "12:\tlearn: 1.0791858\ttotal: 2.39s\tremaining: 3m 1s\n",
      "13:\tlearn: 1.0581286\ttotal: 2.57s\tremaining: 3m 1s\n",
      "14:\tlearn: 1.0376935\ttotal: 2.75s\tremaining: 3m\n",
      "15:\tlearn: 1.0211553\ttotal: 2.92s\tremaining: 2m 59s\n",
      "16:\tlearn: 1.0024868\ttotal: 3.09s\tremaining: 2m 58s\n",
      "17:\tlearn: 0.9857230\ttotal: 3.26s\tremaining: 2m 57s\n",
      "18:\tlearn: 0.9708843\ttotal: 3.43s\tremaining: 2m 57s\n",
      "19:\tlearn: 0.9543657\ttotal: 3.6s\tremaining: 2m 56s\n",
      "20:\tlearn: 0.9403152\ttotal: 3.76s\tremaining: 2m 55s\n",
      "21:\tlearn: 0.9297493\ttotal: 3.92s\tremaining: 2m 54s\n",
      "22:\tlearn: 0.9192523\ttotal: 4.1s\tremaining: 2m 54s\n",
      "23:\tlearn: 0.9119126\ttotal: 4.26s\tremaining: 2m 53s\n",
      "24:\tlearn: 0.8993043\ttotal: 4.42s\tremaining: 2m 52s\n",
      "25:\tlearn: 0.8906152\ttotal: 4.6s\tremaining: 2m 52s\n",
      "26:\tlearn: 0.8778040\ttotal: 4.78s\tremaining: 2m 52s\n",
      "27:\tlearn: 0.8696793\ttotal: 4.96s\tremaining: 2m 52s\n",
      "28:\tlearn: 0.8652976\ttotal: 5.14s\tremaining: 2m 52s\n",
      "29:\tlearn: 0.8586079\ttotal: 5.32s\tremaining: 2m 51s\n",
      "30:\tlearn: 0.8541752\ttotal: 5.49s\tremaining: 2m 51s\n",
      "31:\tlearn: 0.8478443\ttotal: 5.65s\tremaining: 2m 50s\n",
      "32:\tlearn: 0.8374726\ttotal: 5.81s\tremaining: 2m 50s\n",
      "33:\tlearn: 0.8316874\ttotal: 5.98s\tremaining: 2m 49s\n",
      "34:\tlearn: 0.8258938\ttotal: 6.14s\tremaining: 2m 49s\n",
      "35:\tlearn: 0.8189936\ttotal: 6.31s\tremaining: 2m 48s\n",
      "36:\tlearn: 0.8145682\ttotal: 6.47s\tremaining: 2m 48s\n",
      "37:\tlearn: 0.8096758\ttotal: 6.65s\tremaining: 2m 48s\n",
      "38:\tlearn: 0.8023525\ttotal: 6.81s\tremaining: 2m 47s\n",
      "39:\tlearn: 0.7967862\ttotal: 6.98s\tremaining: 2m 47s\n",
      "40:\tlearn: 0.7923403\ttotal: 7.16s\tremaining: 2m 47s\n",
      "41:\tlearn: 0.7878892\ttotal: 7.33s\tremaining: 2m 47s\n",
      "42:\tlearn: 0.7842206\ttotal: 7.5s\tremaining: 2m 46s\n",
      "43:\tlearn: 0.7790467\ttotal: 7.66s\tremaining: 2m 46s\n",
      "44:\tlearn: 0.7748266\ttotal: 7.84s\tremaining: 2m 46s\n",
      "45:\tlearn: 0.7703751\ttotal: 8.02s\tremaining: 2m 46s\n",
      "46:\tlearn: 0.7670667\ttotal: 8.2s\tremaining: 2m 46s\n",
      "47:\tlearn: 0.7631966\ttotal: 8.37s\tremaining: 2m 46s\n",
      "48:\tlearn: 0.7597005\ttotal: 8.55s\tremaining: 2m 45s\n",
      "49:\tlearn: 0.7572528\ttotal: 8.72s\tremaining: 2m 45s\n",
      "50:\tlearn: 0.7540785\ttotal: 8.89s\tremaining: 2m 45s\n",
      "51:\tlearn: 0.7497730\ttotal: 9.07s\tremaining: 2m 45s\n",
      "52:\tlearn: 0.7471485\ttotal: 9.23s\tremaining: 2m 45s\n",
      "53:\tlearn: 0.7442786\ttotal: 9.4s\tremaining: 2m 44s\n",
      "54:\tlearn: 0.7418027\ttotal: 9.57s\tremaining: 2m 44s\n",
      "55:\tlearn: 0.7385968\ttotal: 9.75s\tremaining: 2m 44s\n",
      "56:\tlearn: 0.7340364\ttotal: 9.92s\tremaining: 2m 44s\n",
      "57:\tlearn: 0.7308006\ttotal: 10.1s\tremaining: 2m 43s\n",
      "58:\tlearn: 0.7249499\ttotal: 10.3s\tremaining: 2m 43s\n",
      "59:\tlearn: 0.7219135\ttotal: 10.4s\tremaining: 2m 43s\n",
      "60:\tlearn: 0.7173445\ttotal: 10.6s\tremaining: 2m 43s\n",
      "61:\tlearn: 0.7130680\ttotal: 10.8s\tremaining: 2m 42s\n",
      "62:\tlearn: 0.7111552\ttotal: 10.9s\tremaining: 2m 42s\n",
      "63:\tlearn: 0.7078062\ttotal: 11.1s\tremaining: 2m 42s\n",
      "64:\tlearn: 0.7042355\ttotal: 11.3s\tremaining: 2m 42s\n",
      "65:\tlearn: 0.6988495\ttotal: 11.4s\tremaining: 2m 41s\n",
      "66:\tlearn: 0.6958752\ttotal: 11.6s\tremaining: 2m 41s\n",
      "67:\tlearn: 0.6929474\ttotal: 11.8s\tremaining: 2m 41s\n",
      "68:\tlearn: 0.6906270\ttotal: 12s\tremaining: 2m 41s\n",
      "69:\tlearn: 0.6876921\ttotal: 12.1s\tremaining: 2m 41s\n",
      "70:\tlearn: 0.6853229\ttotal: 12.3s\tremaining: 2m 40s\n",
      "71:\tlearn: 0.6818664\ttotal: 12.5s\tremaining: 2m 40s\n",
      "72:\tlearn: 0.6789622\ttotal: 12.6s\tremaining: 2m 40s\n",
      "73:\tlearn: 0.6762902\ttotal: 12.8s\tremaining: 2m 40s\n",
      "74:\tlearn: 0.6746538\ttotal: 13s\tremaining: 2m 40s\n",
      "75:\tlearn: 0.6715503\ttotal: 13.2s\tremaining: 2m 40s\n",
      "76:\tlearn: 0.6685120\ttotal: 13.3s\tremaining: 2m 39s\n",
      "77:\tlearn: 0.6668837\ttotal: 13.5s\tremaining: 2m 39s\n",
      "78:\tlearn: 0.6640751\ttotal: 13.7s\tremaining: 2m 39s\n",
      "79:\tlearn: 0.6612375\ttotal: 13.8s\tremaining: 2m 39s\n",
      "80:\tlearn: 0.6596475\ttotal: 14s\tremaining: 2m 38s\n",
      "81:\tlearn: 0.6578985\ttotal: 14.2s\tremaining: 2m 38s\n",
      "82:\tlearn: 0.6552904\ttotal: 14.4s\tremaining: 2m 38s\n",
      "83:\tlearn: 0.6535447\ttotal: 14.5s\tremaining: 2m 38s\n",
      "84:\tlearn: 0.6508639\ttotal: 14.7s\tremaining: 2m 38s\n",
      "85:\tlearn: 0.6491895\ttotal: 14.9s\tremaining: 2m 37s\n",
      "86:\tlearn: 0.6477062\ttotal: 15s\tremaining: 2m 37s\n",
      "87:\tlearn: 0.6455211\ttotal: 15.2s\tremaining: 2m 37s\n",
      "88:\tlearn: 0.6440187\ttotal: 15.4s\tremaining: 2m 37s\n",
      "89:\tlearn: 0.6422073\ttotal: 15.5s\tremaining: 2m 37s\n",
      "90:\tlearn: 0.6399336\ttotal: 15.7s\tremaining: 2m 36s\n",
      "91:\tlearn: 0.6372700\ttotal: 15.9s\tremaining: 2m 36s\n",
      "92:\tlearn: 0.6345897\ttotal: 16s\tremaining: 2m 36s\n",
      "93:\tlearn: 0.6331991\ttotal: 16.2s\tremaining: 2m 36s\n",
      "94:\tlearn: 0.6314293\ttotal: 16.4s\tremaining: 2m 36s\n",
      "95:\tlearn: 0.6298067\ttotal: 16.6s\tremaining: 2m 35s\n",
      "96:\tlearn: 0.6283333\ttotal: 16.7s\tremaining: 2m 35s\n",
      "97:\tlearn: 0.6257076\ttotal: 16.9s\tremaining: 2m 35s\n",
      "98:\tlearn: 0.6244854\ttotal: 17.1s\tremaining: 2m 35s\n",
      "99:\tlearn: 0.6230832\ttotal: 17.2s\tremaining: 2m 35s\n",
      "100:\tlearn: 0.6213188\ttotal: 17.4s\tremaining: 2m 34s\n",
      "101:\tlearn: 0.6186853\ttotal: 17.6s\tremaining: 2m 34s\n",
      "102:\tlearn: 0.6169648\ttotal: 17.8s\tremaining: 2m 34s\n",
      "103:\tlearn: 0.6146514\ttotal: 17.9s\tremaining: 2m 34s\n",
      "104:\tlearn: 0.6117954\ttotal: 18.1s\tremaining: 2m 34s\n",
      "105:\tlearn: 0.6097127\ttotal: 18.3s\tremaining: 2m 34s\n",
      "106:\tlearn: 0.6081977\ttotal: 18.5s\tremaining: 2m 34s\n",
      "107:\tlearn: 0.6061884\ttotal: 18.7s\tremaining: 2m 34s\n",
      "108:\tlearn: 0.6046484\ttotal: 18.8s\tremaining: 2m 33s\n",
      "109:\tlearn: 0.6017687\ttotal: 19s\tremaining: 2m 33s\n",
      "110:\tlearn: 0.6002493\ttotal: 19.2s\tremaining: 2m 33s\n",
      "111:\tlearn: 0.5975519\ttotal: 19.4s\tremaining: 2m 33s\n",
      "112:\tlearn: 0.5959106\ttotal: 19.5s\tremaining: 2m 33s\n",
      "113:\tlearn: 0.5940244\ttotal: 19.7s\tremaining: 2m 33s\n",
      "114:\tlearn: 0.5924267\ttotal: 19.9s\tremaining: 2m 32s\n",
      "115:\tlearn: 0.5907442\ttotal: 20s\tremaining: 2m 32s\n",
      "116:\tlearn: 0.5887249\ttotal: 20.2s\tremaining: 2m 32s\n",
      "117:\tlearn: 0.5860384\ttotal: 20.4s\tremaining: 2m 32s\n",
      "118:\tlearn: 0.5841404\ttotal: 20.5s\tremaining: 2m 32s\n",
      "119:\tlearn: 0.5828956\ttotal: 20.7s\tremaining: 2m 31s\n",
      "120:\tlearn: 0.5810549\ttotal: 20.9s\tremaining: 2m 31s\n",
      "121:\tlearn: 0.5798868\ttotal: 21.1s\tremaining: 2m 31s\n",
      "122:\tlearn: 0.5775314\ttotal: 21.2s\tremaining: 2m 31s\n",
      "123:\tlearn: 0.5748178\ttotal: 21.4s\tremaining: 2m 31s\n",
      "124:\tlearn: 0.5723453\ttotal: 21.6s\tremaining: 2m 30s\n",
      "125:\tlearn: 0.5699810\ttotal: 21.7s\tremaining: 2m 30s\n",
      "126:\tlearn: 0.5664502\ttotal: 21.9s\tremaining: 2m 30s\n",
      "127:\tlearn: 0.5644028\ttotal: 22.1s\tremaining: 2m 30s\n",
      "128:\tlearn: 0.5627086\ttotal: 22.2s\tremaining: 2m 30s\n",
      "129:\tlearn: 0.5605284\ttotal: 22.4s\tremaining: 2m 30s\n",
      "130:\tlearn: 0.5585370\ttotal: 22.6s\tremaining: 2m 29s\n",
      "131:\tlearn: 0.5559024\ttotal: 22.8s\tremaining: 2m 29s\n",
      "132:\tlearn: 0.5542858\ttotal: 22.9s\tremaining: 2m 29s\n",
      "133:\tlearn: 0.5535570\ttotal: 23.1s\tremaining: 2m 29s\n",
      "134:\tlearn: 0.5518072\ttotal: 23.3s\tremaining: 2m 29s\n",
      "135:\tlearn: 0.5505490\ttotal: 23.5s\tremaining: 2m 28s\n",
      "136:\tlearn: 0.5491147\ttotal: 23.6s\tremaining: 2m 28s\n",
      "137:\tlearn: 0.5476175\ttotal: 23.8s\tremaining: 2m 28s\n",
      "138:\tlearn: 0.5466090\ttotal: 24s\tremaining: 2m 28s\n",
      "139:\tlearn: 0.5448260\ttotal: 24.1s\tremaining: 2m 28s\n",
      "140:\tlearn: 0.5432159\ttotal: 24.3s\tremaining: 2m 28s\n",
      "141:\tlearn: 0.5420513\ttotal: 24.5s\tremaining: 2m 27s\n",
      "142:\tlearn: 0.5405835\ttotal: 24.7s\tremaining: 2m 27s\n",
      "143:\tlearn: 0.5394656\ttotal: 24.8s\tremaining: 2m 27s\n",
      "144:\tlearn: 0.5379688\ttotal: 25s\tremaining: 2m 27s\n",
      "145:\tlearn: 0.5368768\ttotal: 25.2s\tremaining: 2m 27s\n",
      "146:\tlearn: 0.5355211\ttotal: 25.3s\tremaining: 2m 27s\n",
      "147:\tlearn: 0.5347115\ttotal: 25.5s\tremaining: 2m 26s\n",
      "148:\tlearn: 0.5335561\ttotal: 25.7s\tremaining: 2m 26s\n",
      "149:\tlearn: 0.5324043\ttotal: 25.9s\tremaining: 2m 26s\n",
      "150:\tlearn: 0.5306549\ttotal: 26s\tremaining: 2m 26s\n",
      "151:\tlearn: 0.5297020\ttotal: 26.2s\tremaining: 2m 26s\n",
      "152:\tlearn: 0.5286707\ttotal: 26.4s\tremaining: 2m 26s\n",
      "153:\tlearn: 0.5275557\ttotal: 26.5s\tremaining: 2m 25s\n",
      "154:\tlearn: 0.5263307\ttotal: 26.7s\tremaining: 2m 25s\n",
      "155:\tlearn: 0.5245952\ttotal: 26.9s\tremaining: 2m 25s\n",
      "156:\tlearn: 0.5233596\ttotal: 27.1s\tremaining: 2m 25s\n",
      "157:\tlearn: 0.5218887\ttotal: 27.2s\tremaining: 2m 25s\n",
      "158:\tlearn: 0.5212041\ttotal: 27.4s\tremaining: 2m 24s\n",
      "159:\tlearn: 0.5191751\ttotal: 27.6s\tremaining: 2m 24s\n",
      "160:\tlearn: 0.5178687\ttotal: 27.8s\tremaining: 2m 24s\n",
      "161:\tlearn: 0.5167248\ttotal: 27.9s\tremaining: 2m 24s\n",
      "162:\tlearn: 0.5145039\ttotal: 28.1s\tremaining: 2m 24s\n",
      "163:\tlearn: 0.5133227\ttotal: 28.3s\tremaining: 2m 24s\n",
      "164:\tlearn: 0.5106451\ttotal: 28.4s\tremaining: 2m 23s\n",
      "165:\tlearn: 0.5096830\ttotal: 28.6s\tremaining: 2m 23s\n",
      "166:\tlearn: 0.5085539\ttotal: 28.8s\tremaining: 2m 23s\n",
      "167:\tlearn: 0.5072955\ttotal: 29s\tremaining: 2m 23s\n",
      "168:\tlearn: 0.5063073\ttotal: 29.1s\tremaining: 2m 23s\n",
      "169:\tlearn: 0.5055210\ttotal: 29.3s\tremaining: 2m 23s\n",
      "170:\tlearn: 0.5045321\ttotal: 29.5s\tremaining: 2m 22s\n",
      "171:\tlearn: 0.5034634\ttotal: 29.7s\tremaining: 2m 22s\n",
      "172:\tlearn: 0.5023357\ttotal: 29.8s\tremaining: 2m 22s\n",
      "173:\tlearn: 0.5010829\ttotal: 30s\tremaining: 2m 22s\n",
      "174:\tlearn: 0.4998711\ttotal: 30.2s\tremaining: 2m 22s\n",
      "175:\tlearn: 0.4987583\ttotal: 30.3s\tremaining: 2m 22s\n",
      "176:\tlearn: 0.4976583\ttotal: 30.5s\tremaining: 2m 21s\n",
      "177:\tlearn: 0.4966914\ttotal: 30.7s\tremaining: 2m 21s\n",
      "178:\tlearn: 0.4956565\ttotal: 30.8s\tremaining: 2m 21s\n",
      "179:\tlearn: 0.4944754\ttotal: 31s\tremaining: 2m 21s\n",
      "180:\tlearn: 0.4935023\ttotal: 31.2s\tremaining: 2m 21s\n",
      "181:\tlearn: 0.4926930\ttotal: 31.3s\tremaining: 2m 20s\n",
      "182:\tlearn: 0.4913878\ttotal: 31.5s\tremaining: 2m 20s\n",
      "183:\tlearn: 0.4904724\ttotal: 31.7s\tremaining: 2m 20s\n",
      "184:\tlearn: 0.4894141\ttotal: 31.9s\tremaining: 2m 20s\n",
      "185:\tlearn: 0.4885247\ttotal: 32s\tremaining: 2m 20s\n",
      "186:\tlearn: 0.4874030\ttotal: 32.2s\tremaining: 2m 19s\n",
      "187:\tlearn: 0.4863468\ttotal: 32.4s\tremaining: 2m 19s\n",
      "188:\tlearn: 0.4851784\ttotal: 32.5s\tremaining: 2m 19s\n",
      "189:\tlearn: 0.4841806\ttotal: 32.7s\tremaining: 2m 19s\n",
      "190:\tlearn: 0.4828707\ttotal: 32.9s\tremaining: 2m 19s\n",
      "191:\tlearn: 0.4821715\ttotal: 33.1s\tremaining: 2m 19s\n",
      "192:\tlearn: 0.4812717\ttotal: 33.2s\tremaining: 2m 18s\n",
      "193:\tlearn: 0.4797762\ttotal: 33.4s\tremaining: 2m 18s\n",
      "194:\tlearn: 0.4788854\ttotal: 33.6s\tremaining: 2m 18s\n",
      "195:\tlearn: 0.4776774\ttotal: 33.8s\tremaining: 2m 18s\n",
      "196:\tlearn: 0.4763751\ttotal: 34s\tremaining: 2m 18s\n",
      "197:\tlearn: 0.4755446\ttotal: 34.1s\tremaining: 2m 18s\n",
      "198:\tlearn: 0.4744526\ttotal: 34.3s\tremaining: 2m 18s\n",
      "199:\tlearn: 0.4734904\ttotal: 34.5s\tremaining: 2m 17s\n",
      "200:\tlearn: 0.4722054\ttotal: 34.6s\tremaining: 2m 17s\n",
      "201:\tlearn: 0.4714136\ttotal: 34.8s\tremaining: 2m 17s\n",
      "202:\tlearn: 0.4701566\ttotal: 35s\tremaining: 2m 17s\n",
      "203:\tlearn: 0.4695871\ttotal: 35.1s\tremaining: 2m 17s\n",
      "204:\tlearn: 0.4687801\ttotal: 35.3s\tremaining: 2m 16s\n",
      "205:\tlearn: 0.4682382\ttotal: 35.5s\tremaining: 2m 16s\n",
      "206:\tlearn: 0.4671153\ttotal: 35.6s\tremaining: 2m 16s\n",
      "207:\tlearn: 0.4660045\ttotal: 35.8s\tremaining: 2m 16s\n",
      "208:\tlearn: 0.4652504\ttotal: 36s\tremaining: 2m 16s\n",
      "209:\tlearn: 0.4643234\ttotal: 36.1s\tremaining: 2m 15s\n",
      "210:\tlearn: 0.4634479\ttotal: 36.3s\tremaining: 2m 15s\n",
      "211:\tlearn: 0.4623772\ttotal: 36.5s\tremaining: 2m 15s\n",
      "212:\tlearn: 0.4619881\ttotal: 36.7s\tremaining: 2m 15s\n",
      "213:\tlearn: 0.4611733\ttotal: 36.8s\tremaining: 2m 15s\n",
      "214:\tlearn: 0.4603636\ttotal: 37s\tremaining: 2m 15s\n",
      "215:\tlearn: 0.4595892\ttotal: 37.2s\tremaining: 2m 14s\n",
      "216:\tlearn: 0.4589868\ttotal: 37.3s\tremaining: 2m 14s\n",
      "217:\tlearn: 0.4582033\ttotal: 37.5s\tremaining: 2m 14s\n",
      "218:\tlearn: 0.4572585\ttotal: 37.7s\tremaining: 2m 14s\n",
      "219:\tlearn: 0.4564490\ttotal: 37.9s\tremaining: 2m 14s\n",
      "220:\tlearn: 0.4553537\ttotal: 38s\tremaining: 2m 14s\n",
      "221:\tlearn: 0.4542641\ttotal: 38.2s\tremaining: 2m 13s\n",
      "222:\tlearn: 0.4535735\ttotal: 38.4s\tremaining: 2m 13s\n",
      "223:\tlearn: 0.4530054\ttotal: 38.5s\tremaining: 2m 13s\n",
      "224:\tlearn: 0.4523319\ttotal: 38.7s\tremaining: 2m 13s\n",
      "225:\tlearn: 0.4515478\ttotal: 38.9s\tremaining: 2m 13s\n",
      "226:\tlearn: 0.4507312\ttotal: 39.1s\tremaining: 2m 13s\n",
      "227:\tlearn: 0.4497029\ttotal: 39.2s\tremaining: 2m 12s\n",
      "228:\tlearn: 0.4491276\ttotal: 39.4s\tremaining: 2m 12s\n",
      "229:\tlearn: 0.4481844\ttotal: 39.6s\tremaining: 2m 12s\n",
      "230:\tlearn: 0.4471871\ttotal: 39.8s\tremaining: 2m 12s\n",
      "231:\tlearn: 0.4461732\ttotal: 39.9s\tremaining: 2m 12s\n",
      "232:\tlearn: 0.4451629\ttotal: 40.1s\tremaining: 2m 12s\n",
      "233:\tlearn: 0.4444730\ttotal: 40.3s\tremaining: 2m 11s\n",
      "234:\tlearn: 0.4434889\ttotal: 40.5s\tremaining: 2m 11s\n",
      "235:\tlearn: 0.4427976\ttotal: 40.6s\tremaining: 2m 11s\n",
      "236:\tlearn: 0.4420493\ttotal: 40.8s\tremaining: 2m 11s\n",
      "237:\tlearn: 0.4415138\ttotal: 41s\tremaining: 2m 11s\n",
      "238:\tlearn: 0.4407023\ttotal: 41.2s\tremaining: 2m 11s\n",
      "239:\tlearn: 0.4398123\ttotal: 41.3s\tremaining: 2m 10s\n",
      "240:\tlearn: 0.4387413\ttotal: 41.5s\tremaining: 2m 10s\n",
      "241:\tlearn: 0.4380407\ttotal: 41.7s\tremaining: 2m 10s\n",
      "242:\tlearn: 0.4372164\ttotal: 41.8s\tremaining: 2m 10s\n",
      "243:\tlearn: 0.4367776\ttotal: 42s\tremaining: 2m 10s\n",
      "244:\tlearn: 0.4359208\ttotal: 42.2s\tremaining: 2m 9s\n",
      "245:\tlearn: 0.4353155\ttotal: 42.4s\tremaining: 2m 9s\n",
      "246:\tlearn: 0.4345168\ttotal: 42.5s\tremaining: 2m 9s\n",
      "247:\tlearn: 0.4334290\ttotal: 42.7s\tremaining: 2m 9s\n",
      "248:\tlearn: 0.4324859\ttotal: 42.9s\tremaining: 2m 9s\n",
      "249:\tlearn: 0.4319604\ttotal: 43.1s\tremaining: 2m 9s\n",
      "250:\tlearn: 0.4312059\ttotal: 43.3s\tremaining: 2m 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m X_test_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_test)\n\u001b[0;32m     29\u001b[0m modelcatboost \u001b[39m=\u001b[39m CatBoostClassifier(verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m modelcatboost\u001b[39m.\u001b[39;49mfit(X_train_scaled, y_train)\n\u001b[0;32m     31\u001b[0m y_pred \u001b[39m=\u001b[39m modelcatboost\u001b[39m.\u001b[39mpredict(X_test_scaled)\n\u001b[0;32m     32\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(y_true\u001b[39m=\u001b[39my_test, y_pred\u001b[39m=\u001b[39my_pred)\n",
      "File \u001b[1;32mc:\\Users\\defeo\\anaconda3\\envs\\deepunet\\lib\\site-packages\\catboost\\core.py:5131\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[0;32m   5129\u001b[0m     CatBoostClassifier\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m-> 5131\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline, use_best_model,\n\u001b[0;32m   5132\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[0;32m   5133\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[0;32m   5134\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\defeo\\anaconda3\\envs\\deepunet\\lib\\site-packages\\catboost\\core.py:2357\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2353\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2355\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[0;32m   2356\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2357\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[0;32m   2358\u001b[0m         train_pool,\n\u001b[0;32m   2359\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   2360\u001b[0m         params,\n\u001b[0;32m   2361\u001b[0m         allow_clear_pool,\n\u001b[0;32m   2362\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m   2363\u001b[0m     )\n\u001b[0;32m   2365\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2366\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mc:\\Users\\defeo\\anaconda3\\envs\\deepunet\\lib\\site-packages\\catboost\\core.py:1761\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1761\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   1762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4624\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4673\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy_scores = []\n",
    "f1_scores_list = []\n",
    "recall_scores = []\n",
    "precision_scores_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    # Generate a list of unique IDs from your features\n",
    "    unique_ids = data[0].unique()\n",
    "\n",
    "    # Randomly select 10 unique IDs\n",
    "    random_ids = pd.Series(unique_ids).sample(90).tolist()\n",
    "\n",
    "    # Filter the features\n",
    "    random_10 = data[data[0].isin(random_ids)]\n",
    "\n",
    "\n",
    "    # Extract the features and target variable from the sample\n",
    "    X = random_10.drop([0,473], axis=1)\n",
    "    y = random_10[473]\n",
    "\n",
    "    # Change labels from 1-index based to 0-index based (reason: features imported from matlab)\n",
    "    y=y-1\n",
    "\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X.fillna(0,inplace=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42,shuffle=False)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    modelcatboost = CatBoostClassifier(verbose=1)\n",
    "    modelcatboost.fit(X_train_scaled, y_train)\n",
    "    y_pred = modelcatboost.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1 = f1_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    f1_scores_list.append(f1)\n",
    "    recall = recall_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    recall_scores.append(recall)\n",
    "    precision = precision_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    precision_scores_list.append(precision)\n",
    "\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "mean_f1 = np.mean(f1_scores_list)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_precision = np.mean(precision_scores_list)\n",
    "\n",
    "catboost_score = {\"Accuracy\": mean_accuracy, \"F1-Score\": mean_f1, \"Recall\": mean_recall, \"Precision\": mean_precision}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h1>SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = []\n",
    "f1_scores_list = []\n",
    "recall_scores = []\n",
    "precision_scores_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    # Generate a list of unique IDs from your features\n",
    "    unique_ids = data[0].unique()\n",
    "\n",
    "    # Randomly select 10 unique IDs\n",
    "    random_ids = pd.Series(unique_ids).sample(20).tolist()\n",
    "\n",
    "    # Filter the features\n",
    "    random_10 = data[data[0].isin(random_ids)]\n",
    "\n",
    "\n",
    "    # Extract the features and target variable from the sample\n",
    "    X = random_10.drop([0,473], axis=1)\n",
    "    y = random_10[473]\n",
    "\n",
    "    # Change labels from 1-index based to 0-index based (reason: features imported from matlab)\n",
    "    y=y-1\n",
    "\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X.fillna(0,inplace=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42,shuffle=False)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    modelSVC= SVC(verbose=True)\n",
    "    modelSVC.fit(X_train_scaled, y_train)\n",
    "    y_pred = modelSVC.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1 = f1_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    f1_scores_list.append(f1)\n",
    "    recall = recall_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    recall_scores.append(recall)\n",
    "    precision = precision_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    precision_scores_list.append(precision)\n",
    "\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "mean_f1 = np.mean(f1_scores_list)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_precision = np.mean(precision_scores_list)\n",
    "\n",
    "SVM_score = {\"Accuracy\": mean_accuracy, \"F1-Score\": mean_f1, \"Recall\": mean_recall, \"Precision\": mean_precision}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = []\n",
    "f1_scores_list = []\n",
    "recall_scores = []\n",
    "precision_scores_list = []\n",
    "for i in tqdm(range(5)):\n",
    "    # Generate a list of unique IDs from your features\n",
    "    unique_ids = data[0].unique()\n",
    "\n",
    "    # Randomly select 10 unique IDs\n",
    "    random_ids = pd.Series(unique_ids).sample(20).tolist()\n",
    "\n",
    "    # Filter the features\n",
    "    random_10 = data[data[0].isin(random_ids)]\n",
    "\n",
    "\n",
    "    # Extract the features and target variable from the sample\n",
    "    X = random_10.drop([0,473], axis=1)\n",
    "    y = random_10[473]\n",
    "\n",
    "    # Change labels from 1-index based to 0-index based (reason: features imported from matlab)\n",
    "    y=y-1\n",
    "\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X.fillna(0,inplace=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42,shuffle=False)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    modelKn= make_pipeline(StandardScaler(),\n",
    "                   KNeighborsClassifier(n_neighbors=20))\n",
    "    modelKn.fit(X_train_scaled, y_train)\n",
    "    y_pred = modelKn.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1 = f1_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    f1_scores_list.append(f1)\n",
    "    recall = recall_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    recall_scores.append(recall)\n",
    "    precision = precision_score(y_true=y_test, y_pred=y_pred, average='macro')\n",
    "    precision_scores_list.append(precision)\n",
    "\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "mean_f1 = np.mean(f1_scores_list)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_precision = np.mean(precision_scores_list)\n",
    "\n",
    "Kn_score = {\"Accuracy\": mean_accuracy, \"F1-Score\": mean_f1, \"Recall\": mean_recall, \"Precision\": mean_precision}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score={\"XGBOOST Model\":xgboost_score,\"CATBOOST Model\":catboost_score,\"SVM Model\":SVM_score,\"KNeighborsClassifier Model\":Kn_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=pd.featuresFrame.from_dict(score, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlighted_score = score.style.highlight_max(color=\"blue\").highlight_min(color=\"red\")\n",
    "highlighted_score.set_caption(\"Lowest values in red, highest values in blue\")\n",
    "display(highlighted_score)\n",
    "\n",
    "highlighted_df = score.style.highlight_max(color='blue').highlight_min(color='red')\n",
    "cm = sns.diverging_palette(10, 220, sep=80, n=7, as_cmap=True)\n",
    "highlighted_df.background_gradient(cmap=cm).set_caption(\"Lowest values in red, highest values in blue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = score['Accuracy'].plot(kind='bar', figsize=(8,6), color='blue')\n",
    "ax.set_title('Accuracy by Model')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = score['F1-Score'].plot(kind='bar', figsize=(8,6), color='red')\n",
    "ax.set_title('f1-score by Model')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('f1-score')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
